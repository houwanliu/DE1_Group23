{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81a25b6c-b90e-4b5f-947c-228bbfc02425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting test with 1 nodes。and 10.0% of the data - Run 1/1\n",
      "Running test with 1 executors and 10.0% of the data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 1, Data Size: 83394 records, Execution Time: 89.79 seconds\n",
      "1      | 1   | 89.79      | 83394     \n",
      "\n",
      "Starting test with 2 nodes。and 20.0% of the data - Run 1/1\n",
      "Running test with 2 executors and 20.0% of the data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 2, Data Size: 166807 records, Execution Time: 121.96 seconds\n",
      "2      | 1   | 121.96     | 166807    \n",
      "\n",
      "Starting test with 3 nodes。and 30.000000000000004% of the data - Run 1/1\n",
      "Running test with 3 executors and 30.000000000000004% of the data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 3, Data Size: 249480 records, Execution Time: 263.39 seconds\n",
      "3      | 1   | 263.39     | 249480    \n",
      "\n",
      "Starting test with 4 nodes。and 40.0% of the data - Run 1/1\n",
      "Running test with 4 executors and 40.0% of the data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 4, Data Size: 332754 records, Execution Time: 151.87 seconds\n",
      "4      | 1   | 151.87     | 332754    \n",
      "\n",
      "Starting test with 5 nodes。and 50.0% of the data - Run 1/1\n",
      "Running test with 5 executors and 50.0% of the data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 5, Data Size: 415956 records, Execution Time: 159.82 seconds\n",
      "5      | 1   | 159.82     | 415956    \n",
      "\n",
      "Starting test with 6 nodes。and 60.00000000000001% of the data - Run 1/1\n",
      "Running test with 6 executors and 60.00000000000001% of the data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 6, Data Size: 499448 records, Execution Time: 180.20 seconds\n",
      "6      | 1   | 180.20     | 499448    \n",
      "\n",
      "Starting test with 7 nodes。and 70.0% of the data - Run 1/1\n",
      "Running test with 7 executors and 70.0% of the data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 7, Data Size: 582711 records, Execution Time: 124.88 seconds\n",
      "7      | 1   | 124.88     | 582711    \n",
      "\n",
      "Starting test with 8 nodes。and 80.0% of the data - Run 1/1\n",
      "Running test with 8 executors and 80.0% of the data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 8, Data Size: 666002 records, Execution Time: 131.64 seconds\n",
      "8      | 1   | 131.64     | 666002    \n",
      "Nodes: 1, Data Fraction: 10.0% | Time: 89.79s\n",
      "Nodes: 2, Data Fraction: 20.0% | Time: 121.96s\n",
      "Nodes: 3, Data Fraction: 30.000000000000004% | Time: 263.39s\n",
      "Nodes: 4, Data Fraction: 40.0% | Time: 151.87s\n",
      "Nodes: 5, Data Fraction: 50.0% | Time: 159.82s\n",
      "Nodes: 6, Data Fraction: 60.00000000000001% | Time: 180.20s\n",
      "Nodes: 7, Data Fraction: 70.0% | Time: 124.88s\n",
      "Nodes: 8, Data Fraction: 80.0% | Time: 131.64s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, length, count, avg, desc\n",
    "\n",
    "\n",
    "def weak_test(num_nodes, data_fraction, hdfs_host=\"192.168.2.156\", master_host=\"192.168.2.156\"):\n",
    "    conf = SparkConf().setAppName(f\"Reddit-Webis-TLDR-17 Analysis - {num_nodes} Nodes\") \\\n",
    "        .set(\"spark.executor.instances\", str(num_nodes))\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(f\"spark://{master_host}:7077\") \\\n",
    "        .config(conf=conf) \\\n",
    "        .appName(\"Reddit-Webis-TLDR-17 Analysis\")\\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(f\"Running test with {num_nodes} executors and {data_fraction*100}% of the data\")\n",
    "    \n",
    "    data_path = \"hdfs://192.168.2.235:9000/user/ubuntu/corpus-webis-tldr-17.json\"\n",
    "    \n",
    "    df = spark.read.json(data_path)\n",
    "    df_sampled = df.sample(fraction=data_fraction, seed=42)  \n",
    "    \n",
    "    start_time = time.time()\n",
    "    df_sampled = df_sampled.dropna(subset=[\"content\", \"summary\"])\n",
    "    df_sampled = df_sampled.withColumn(\"content_length\", length(col(\"content\")))\n",
    "    df_sampled = df_sampled.withColumn(\"summary_length\", length(col(\"summary\")))\n",
    "    record_count = df_sampled.count()  \n",
    "    desc_stats = df_sampled.select(\n",
    "        avg(\"content_length\").alias(\"avg_content_length\"),\n",
    "        avg(\"summary_length\").alias(\"avg_summary_length\")\n",
    "    ).toPandas()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Nodes: {num_nodes}, Data Size: {record_count} records, Execution Time: {execution_time:.2f} seconds\")\n",
    "    spark.stop()\n",
    "    \n",
    "    return execution_time, record_count\n",
    "\n",
    "def run_scaling_tests(node_counts=[1, 2,3, 4,5,6,7, 8], base_fraction=0.1, runs_per_config=1):\n",
    "    results = {}\n",
    "    \n",
    "    for i, nodes in enumerate(node_counts):\n",
    "        fraction = base_fraction * nodes\n",
    "        \n",
    "        if fraction > 1:\n",
    "            print(f\"Warning: Fraction {fraction} exceeds 1, capping at 1.0\")\n",
    "            fraction = 1.0\n",
    "            \n",
    "        times = []\n",
    "        for run in range(1, runs_per_config + 1):  \n",
    "            print(f\"\\nStarting test with {nodes} nodes。and {fraction*100}% of the data - Run {run}/{runs_per_config}\")\n",
    "            time_taken, records = weak_test(nodes, data_fraction=fraction)\n",
    "            times.append(time_taken)\n",
    "            print(f\"{nodes:<6} | {run:<3} | {time_taken:<10.2f} | {records:<10}\")\n",
    "        \n",
    "        avg_time = sum(times) / len(times)\n",
    "        results[(nodes, fraction)] = avg_time\n",
    "    \n",
    "    for (nodes, fraction), time in results.items():\n",
    "        print(f\"Nodes: {nodes}, Data Fraction: {fraction*100}% | Time: {time:.2f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "scaling_results = run_scaling_tests(node_counts=[1, 2, 3,4,5,6,7, 8], base_fraction=0.1, runs_per_config=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5274fc9a-5ccc-4d9b-97f0-ab00f395350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "nodes = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "ideal_efficiency = [100] * len(nodes)\n",
    "time = [89.79, 121.96, 263.39, 151.87, 159.82, 180.20, 124.88, 131.64]\n",
    "actual_efficiency = [execution_time[0] / (p * time[i]) for i, p in enumerate(nodes)]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(nodes, ideal_efficiency, 'r--', label='Ideal Efficiency (100%)', linewidth=2)\n",
    "plt.plot(nodes, actual_efficiency, 'bo-', label='Actual Efficiency', linewidth=2, markersize=8)\n",
    "\n",
    "plt.xlabel(\"Number of Nodes\", fontsize=12)\n",
    "plt.ylabel(\"Efficiency (%)\", fontsize=12)\n",
    "plt.title(\"Weak Scalability Efficiency\", fontsize=14)\n",
    "plt.xticks(nodes)\n",
    "plt.ylim(0, 110)  \n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c76c9d9-eed6-4706-a19e-07ab87926ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b25b970-f2b5-49a6-9d94-c2a673f965d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
